{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.history_aware_retriever import (\n",
    "    create_history_aware_retriever,\n",
    ")\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the llm by these..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment a block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"metrics\"\n",
    "chunk_size=1000\n",
    "chunk_overlap=200\n",
    "llm = ChatOpenAI(model_name=model_name)\n",
    "evaluation_file_name = \"evaluation.json\"\n",
    "\n",
    "# index_name = \"metricsimproved\"\n",
    "# chunk_size=80\n",
    "# chunk_overlap=15\n",
    "# llm = ChatOpenAI(model_name=model_name)\n",
    "# evaluation_file_name = \"evaluation-improved.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1608 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader(\"mongodb-docs/www.mongodb.com\")\n",
    "raw_documents = loader.load()\n",
    "print(f\"loaded {len(raw_documents)} documents\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "docs = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is MongoDB?\",\n",
    "    \"How do I create an index in mongodb?\",\n",
    "    \"What can mongodb do for my app?\",\n",
    "    \"What is Atlas?\",\n",
    "    \"What is database sharding?\",\n",
    "    \"What is useState?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(query: str, chat_history: List[Dict[str, Any]] = []):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    docsearch = PineconeVectorStore(\n",
    "        index_name=index_name, embedding=embeddings\n",
    "    )\n",
    "\n",
    "    rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\")\n",
    "\n",
    "    retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm, retrieval_qa_chat_prompt\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=llm, retriever=docsearch.as_retriever(), prompt=rephrase_prompt\n",
    "    )\n",
    "    qa = create_retrieval_chain(\n",
    "        retriever=history_aware_retriever,\n",
    "        combine_docs_chain=stuff_documents_chain,\n",
    "    )\n",
    "\n",
    "    result = qa.invoke(input={\"input\": query, \"chat_history\": chat_history})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_data(query):\n",
    "    retrieval_results = docsearch.similarity_search(query)\n",
    "    contexts = [result.page_content for result in retrieval_results]\n",
    "\n",
    "    result = run_llm(query, [])\n",
    "    # result = rag_chain.invoke(query)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": result[\"answer\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = [generate_evaluation_data(query) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_context_relevance(questions, retrieved_contexts):\n",
    "    vectorizer = TfidfVectorizer().fit_transform(retrieved_contexts + questions)\n",
    "    vectors = vectorizer.toarray()\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        context_vector = vectors[i]\n",
    "        question_vector = vectors[len(questions) + i]\n",
    "        cosine_sim = cosine_similarity([context_vector], [question_vector])[0][0]\n",
    "        relevance_scores.append(cosine_sim)\n",
    "    \n",
    "    return relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_answer_relevance(answers_from_llm, reference_answers):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for generated, reference in zip(answers_from_llm, reference_answers):\n",
    "        scores = scorer.score(reference, generated)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers generated by GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_answers = [\n",
    "    \"MongoDB is a NoSQL, document-oriented database that stores data in flexible, JSON-like documents. It is designed for scalability, high performance, and ease of development.\",\n",
    "    \"Use the createIndex method on a collection, specifying the field to index, like db.collection.createIndex({ fieldName: 1 }). This improves query performance by allowing the database to quickly locate data.\",\n",
    "    \"MongoDB offers flexible schema design, high scalability, and robust performance, making it ideal for handling large datasets, real-time applications, and rapidly evolving data models. It also supports advanced features like geospatial queries and full-text search.\",\n",
    "    \"MongoDB Atlas is a fully managed cloud database service that automates database provisioning, setup, and maintenance. It provides features like automated backups, scaling, and high availability.\",\n",
    "    \"Database sharding is the process of distributing a single database across multiple servers to improve performance and scalability. Each shard contains a subset of the database's data, enabling the system to handle larger datasets and higher traffic loads.\",\n",
    "    \"useState is a Hook in React that allows you to add state management to functional components. Introduced in React 16.8, Hooks provide a way to use state and other React features without writing class components. useState is used to declare state variables in functional components, enabling them to maintain and update their own state.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_contexts = []\n",
    "for data in evaluation:\n",
    "    retrieved_contexts.append(data['contexts'][0])\n",
    "\n",
    "answers_from_llm = []\n",
    "for data in evaluation:\n",
    "    answers_from_llm.append(data['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context relevance, answer relevance and noise robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': [0.48000000000000004,\n",
       "  0.27027027027027023,\n",
       "  0.37500000000000006,\n",
       "  0.31249999999999994,\n",
       "  0.3125,\n",
       "  0.2162162162162162],\n",
       " 'rouge2': [0.12499999999999997,\n",
       "  0.10958904109589043,\n",
       "  0.1090909090909091,\n",
       "  0.0425531914893617,\n",
       "  0.06451612903225808,\n",
       "  0.0],\n",
       " 'rougeL': [0.32,\n",
       "  0.21621621621621623,\n",
       "  0.23214285714285718,\n",
       "  0.18749999999999997,\n",
       "  0.25,\n",
       "  0.12612612612612611]}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevance_scores = evaluate_context_relevance(queries, retrieved_contexts)\n",
    "context_relevance_scores\n",
    "\n",
    "answer_relevance_scores = evaluate_answer_relevance(answers_from_llm, reference_answers)\n",
    "answer_relevance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {\n",
    "    \"answers_from_llm\": answers_from_llm,\n",
    "    \"context_relevance_scores\": context_relevance_scores,\n",
    "    \"answer_relevance_scores\": answer_relevance_scores\n",
    "}\n",
    "\n",
    "with open(evaluation_file_name, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
